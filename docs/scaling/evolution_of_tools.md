---
layout: default
title: Evolution of the platform and tools
parent: Scaling. Grow your community
nav_order: 4
---

# Evolution of the platform and tools
{: .no_toc }

## Table of contents
{: .no_toc .text-delta }

1. TOC
{:toc}

The requirements for the platform at a particular stage of community development depend on the number of active users in the community and the initiatives being undertaken. If you plan to grow your community big (thousands of active users), at some point you will need to be able to create sub-communities, have advanced moderation tools, and have multiple community roles for volunteers. 

Modern community platforms often have all the necessary modules. All you need is to activate the modules when needed, set the platform settings correctly and manage the access rights of volunteers and moderators. When choosing the platform for your community, please keep in mind that it is very difficult to transfer a community from one platform to another. Launch your community on a platform that is flexible enough to support communities of different sizes.

## The ability to create sub-communities

The ability to create isolated spaces that are at the same time interconnected into a single whole is one of the most critical features that a community platform must provide if you want to run a large community on it, because communities grow through dividing a larger community into sub-communities. 

Any sub-community is a community in itself and, as a result, the dedicated sub-community space should provide users the ability to customize it. The more customization options for dedicated spaces the platform provides, the more effective the growth of sub-communities will be. Here is a minimum one needs to look for:

- Users should be able to define what is on-topic for the sub-community.
- There should be a place for metaâ€“discussions dedicated only to the sub-community.
- There should be a possibility to have moderators for the sub-community.

Give the preference to a platform that will also provide sub-communities with:

- The ability to create their own help center, at least in some minimal form.
- Ability to customize the dedicated space visually.

## Supporting multiple community roles

### Each critical volunteering activity in the community must have its own dedicated role

People become volunteers in some activities if performing the activities brings them pleasure in itself. Separating an activity into a dedicated official role with a special social status is a way to reward volunteers for their contribution to the community. It also establishes some kind of social contract between you and the volunteers in a given role about what is expected from both parties. It also helps to automate managing access of the necessary rights and tools. The more different dedicated roles you have in your community, the more users will take the opportunity to help you grow your community.

An example of a dedicated role in a community is the role of moderator.

### Be sure that the platform provides badges for temporary volunteering activities 

In any community there are always short-term or one-time activities that do not require a dedicated volunteering role. Like helping to facilitate a meetup, speaking at a conference on behalf of the community or participating in a seasonal initiative. At the same time, any such contributions should be rewarded in some way. One of the best ways to do that is to reward volunteers with some special visible badge dedicated to specific initiatives that make the participating volunteers distinguishable from the rest of the community. 

It is impossible to plan in advance all initiatives that you will run. It is very important that the selected platform supports the ability to create custom badges and award them to a group of users based on some criteria. 

## Advanced moderation tools

### Community and content moderation

Be aware that there are two distinct types of moderation in online communities:

- *Content moderation*. All kinds of actions whose goal is to keep the content clean and relevant. Any user should be able to take part in content moderation to some extent, as long as they have a personal interest. Usually, content moderation does not require any special knowledge besides good writing skills and basic common sense. 

- *Moderation of relationships between users*. Moderating relationships requires special skills such as empathy, active listening, conflict resolution, etc. Moderating relationships implies the need to make decisions on very subjective topics. This type of moderation should be restricted only to volunteers who have been explicitly elected to serve these needs either by the community itself or assigned by a community manager.

Each of those types of moderation requires a different moderation tool set with very different access levels. When choosing a platform, pay attention to if moderation activities cover both types of moderation, are broken down into the minimum possible actions and there are some settings tuning which you can control access to moderation tools for the users based on some criteria.

### Moderation is something people should do

People tend to distrust platforms where moderation decisions are done by algorithms. Rule violations always are counter-intuitive for the users who made them, at least on an emotional level. When the decision is made by algorithm there is nobody to appeal to or even simply talk about the case. This makes the users feel like they were treated unfairly. The final decision on whether some moderation action is necessary should always be made by real people. Automation is possible, though, in some exceptional cases, like when algorithms help to identify content that should be reviewed by the users or spam detention and removal.

### Monitor the quality of content and moderation

In a large community, it is impossible to view all content published on the platform due to volume. In order to have an objective view of the quality of the content and how users interact with one another, your platform should provide:

- Content evaluation tools that can estimate the quality of the entire content on the platform based on randomly selected manually evaluated small pieces of content.

- Analytics and metrics that automatically track content health.

There will always be those who deliberately commit sabotage activities in a community, moderation activities are not an exception. The platform should also support tools for monitoring the quality of moderation itself. The platform should allow admins to identify users who are destructive and automatically limit their actions.
